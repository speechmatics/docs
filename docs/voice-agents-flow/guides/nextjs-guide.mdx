---
sidebar_label: NextJS
title: Build a conversational AI web app with Next.js and Flow
---

import { Box, Card, Flex } from "@radix-ui/themes";
import CodeBlock from "@theme/CodeBlock";
import { GithubIcon } from "lucide-react";

{/* -------------- Step 1-------------- */}

import postcssConfigExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-1/postcss.config.mjs";
import globalsCssExampleStepOne from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-1/app/globals.css";

{/* -------------- Step 2 -------------- */}
import providersStepTwo from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-2/app/providers.tsx";
import useAudioContextsExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-2/hooks/useAudioContexts.ts";
import pageStepTwoExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-2/app/page.tsx";
import nextjsConfigExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-2/next.config.ts";

{/* -------------- Step 3 -------------- */}
import globalsCssExampleStepThree from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-3/app/globals.css";
import controlsExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-3/components/Controls.tsx";
import serverActionExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-3/app/actions.ts";
import microphoneSelectExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-3/components/MicrophoneSelect.tsx";
import statusExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-3/components/Status.tsx";
import transcriptViewExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-3/components/TranscriptView.tsx";
import pageStepThreeExample from "?url=https://raw.githubusercontent.com/speechmatics/nextjs-flow-guide/refs/heads/step-3/app/page.tsx";


# Build a conversational AI web app with Next.js and Flow

In this guide, we will walk you through the process of building a conversational AI web application using Next.js and Flow.
You will learn how to set up your development environment, create a Next.js project, integrate Flow and implement a simple conversational AI feature.

You can find the complete code on [GitHub <GithubIcon size={16} />](https://github.com/speechmatics/nextjs-flow-guide).

## Prerequisites

Before getting started, ensure you have:

- [Node.js 20](https://nodejs.org/en) or later
- A Speechmatics account and API key

## Step 1: Setup project, dependencies and API key

We will be using NextJS 15 with App Router and Typescript. We will also use TailwindCSS for styling, but feel free to use any styling solution you prefer.

### Create a new Next.js project

```sh
npx create-next-app@latest nextjs-flow-guide --typescript --eslint --app
```

### Install Speechmatics packages

```sh
# Official Flow API client for React
npm install @speechmatics/flow-client-react

# Used for requesting JWTs for API authentication
npm install @speechmatics/auth

# These let us capture and play raw audio in the browser easily
npm install @speechmatics/browser-audio-input
npm install @speechmatics/browser-audio-input-react
npm install @speechmatics/web-pcm-player-react

# Utility package for rendering the transcript of the conversation
npm install @speechmatics/use-flow-transcript

```

### Install TailwindCSS

These steps are from the[Tailwind docs here](https://tailwindcss.com/docs/installation/framework-guides/nextjs).

1. Install TailwindCSS
```sh
npm install @tailwindcss/postcss
```

2. Create a `postcss.config.mjs` file in the root of the project with the following content:

<CodeBlock title="postcss.config.mjs" language="js">
  {postcssConfigExample}
</CodeBlock>

Finally, remove all styles from the `globals.css` file, and replace it with the following content:

<CodeBlock title="app/globals.css" language="css">
  {globalsCssExampleStepOne}
</CodeBlock>


### Add your API key to `.env`

Create a `.env` file in the root of the project, and add your API key:

<CodeBlock title=".env" language="bash" showLineNumbers>
  {"API_KEY=\"your-api-key\""}
</CodeBlock>

## Step 2: Configuration and Context providers

### Configure Webpack to serve AudioWorklet script

To interface with the Flow API, we need to record and send raw audio. The `@speechmatics/browser-audio-input` package is designed to do this. It achieves this by providing a script which can be loaded by an [AudioWorklet](https://developer.mozilla.org/en-US/docs/Web/API/AudioWorklet), but how this script is consumed depends on the bundler being used.

In order to use this package with NextJS, we need to configure Webpack to serve the provided script from a URL, rather than bundling it with the rest. We can leverage [Asset Modules](https://webpack.js.org/guides/asset-modules/) to achieve this.

<CodeBlock title="next.config.ts" language="ts" showLineNumbers>
  {nextjsConfigExample}
</CodeBlock>


### Audio and Context providers

#### Context providers

We will be using 3 context providers in the app:

1. **FlowProvider** - Provides the Flow client to the app.
2. **PCMAudioRecorderProvider** - Given an `AudioContext`, provides the browser audio input to the app.
3. **PCMAudioPlayerProvider** - Given an `AudioContext`, provides the web PCM player to the app.

We'll start by creating a `providers.tsx` file in the `app` directory, and adding the following content:

Here we add the 3 context providers to the app, passing the `AudioContext` instances to both the audio providers, and the `workletScriptURL` to `PCMAudioRecorderProvider`.
<CodeBlock title="app/providers.tsx" language="tsx" showLineNumbers>
  {providersStepTwo}
</CodeBlock>

:::info
#### A note about `AudioContext` and sample rates in Firefox

[AudioContext](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext) is the WebAPI for handling audio recording and playback. Under normal circumstances, you should aim to have [one reusable instance](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext#:~:text=It%27s%20recommended%20to%20create%20one%20AudioContext%20and%20reuse%20it%20instead%20of%20initializing%20a%20new%20one%20each%20time) of an `AudioContext` in your app. In most browsers, an `AudioContext` can freely record and play back audio at different sample rates, but this is not the case in Firefox (see outstanding [bug here](https://bugzilla.mozilla.org/show_bug.cgi?id=1725336)).


To handle this, we can create a utility hook to expose separate `AudioContext` instances for recording and playback in Firefox, while sharing a single instance for other browsers (see below).

:::

<CodeBlock title="hooks/useAudioContexts.ts" language="tsx" showLineNumbers>
  {useAudioContextsExample}
</CodeBlock>

Now place the following code in the `app/page.tsx` file:

<CodeBlock title="app/page.tsx" language="tsx" showLineNumbers>
  {pageStepTwoExample}
</CodeBlock>

:::tip
If you get an error about `AudioWorkletProcessor` not being defined, make sure you [configured Webpack to serve the script URL](#configure-webpack-to-serve-audioworklet-script).
:::

## Step 3: Implementing the UI


### Wireframe and styles

The UI will follow this wireframe:

<Box asChild mb="4">
  <Card>
    <Flex direction="column" gap="1">
      <Flex width="100%" gap="1">
        <Box asChild width="100%">
          <Card>
            <p>**Controls**</p>
            <p>Where we can select the input device and persona, and start/stop the session.</p>
          </Card>
        </Box>
        <Box asChild width="100%">
          <Card>
            <p>**Status**</p>
            <p>Displays the current status of the connection.</p>
          </Card>
        </Box>
      </Flex>
        <Card>
          <Flex direction="column" width="100%"  justify="center" align="center" style={{ height: "200px" }}>
            <p>**TranscriptView**</p>
            <p>Displays the transcript of the conversation.</p>
          </Flex>
        </Card>
    </Flex>
  </Card>
</Box>

We'll start by adding some basic styles to the `app/globals.css` file:

<CodeBlock title="app/globals.css" language="css">
  {globalsCssExampleStepThree}
</CodeBlock>


### `Controls` component

This component will contain:

- A dropdown to select the input device
- A dropdown to select the persona
- A button to start/stop the session
- A button to mute the microphone when the session is active

To connect to the API, we will also need to setup a [Server Action](https://react.dev/reference/rsc/server-functions) to request a JWT from the backend. We can then call this server action in our component.

Here we define the controls component:
  - `form` contains the dropdowns and buttons
  - When the form is submitted, we call the `getJWT` server action, then pass the JWT to the `startConversation` function, along with the config from the FormData.

<CodeBlock title="components/Controls.tsx" language="tsx" showLineNumbers>
  {controlsExample}
</CodeBlock>

We also create a utility component to render the microphone select dropdown. It also handles prompting the user for permission to use the microphone.
<CodeBlock title="components/MicrophoneSelect.tsx" language="tsx" showLineNumbers>
  {microphoneSelectExample}
</CodeBlock>

Finally we define the server action to request a JWT from the backend.
<CodeBlock title="app/actions.ts" language="ts" showLineNumbers>
  {serverActionExample}
</CodeBlock>


### `Status` component

This component will display:
  - The status of the Websocket connection
  - The Session ID of the current conversation
  - Whether the microphone is recording

<CodeBlock title="components/Status.tsx" language="tsx" showLineNumbers>
  {statusExample}
</CodeBlock>


### `TranscriptView` component

This component will use the `useFlowTranscript` hook to display the transcript of the conversation.

:::tip
The `useFlowTranscript` hook is provided for convenience. If you want more fine-grained control over the transcript you should use the `useFlowEventListener` hook to listen for incoming events, and handle them as you see fit.
:::

<CodeBlock title="components/TranscriptView.tsx" language="tsx" showLineNumbers>
  {transcriptViewExample}
</CodeBlock>

### Putting it all together

Now we can update the `app/page.tsx` file to use the new components:

:::note
Since the component in `page.tsx` is a [React Server Component](https://react.dev/reference/rsc/server-components), we can use it to fetch the list of personas from the backend, and pass it to the `Controls` component.
:::

<CodeBlock title="app/page.tsx" language="tsx" showLineNumbers>
  {pageStepThreeExample}
</CodeBlock>

## Running the app

To run the app, use the following command:

```sh
npm run dev
```

You should now be able to access the app at [`http://localhost:3000`](http://localhost:3000).