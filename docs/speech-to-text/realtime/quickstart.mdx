---
description: Learn how to convert streaming audio to text.
---

import Admonition from '@theme/Admonition';
import CodeBlock from '@theme/CodeBlock';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

import javascriptRadioExample from "./assets/javascript-radio-example.js?raw"
import pythonRadioExample from "./assets/url-example.py?raw"

# Quickstart

:::tip
The easiest way to try Realtime transcription is via the [web portal](https://portal.speechmatics.com/jobs/create/real-time).
:::

## Using the Realtime SaaS webSocket API

### 1. Create an API key

[Create an API key in the portal here](https://portal.speechmatics.com/settings/api-keys), which you'll use to securely access the API.
Store the key as a managed secret.

:::info
Enterprise customers may need to speak to [Support](https://support.speechmatics.com) to get your API keys.
:::

### 2. Understand authentication

Realtime transcription uses WebSocket connections, and how you authenticate depends on where your code runs:

- **Server-side (Node.js, Python, etc.):** You can use your API key directly in the `Authorization` header of the WebSocket upgrade request. No extra steps needed.
- **Client-side (browser):** You must generate a short-lived temporary key (JWT) and pass it as a query parameter on the WebSocket URL. This is because browsers cannot set custom headers on WebSocket connections, and it avoids exposing your long-lived API key to end users.

**How the SDKs handle this:**
- The **Python SDK** sends your API key directly by default. You can also set `generate_temp_token=True` in `ConnectionSettings` to have the SDK automatically generate a temporary key before connecting.
- The **JavaScript SDK** always uses a temporary key. Use the `@speechmatics/auth` package to generate one from your API key before connecting.

If you are building a custom WebSocket client (without an SDK) and need a temporary key, generate one with this `curl` command:

```bash
curl -L -X POST "https://mp.speechmatics.com/v1/api_keys?type=rt" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer $API_KEY" \
     -d '{"ttl": 60}'
```

The response will contain a `key_value` field with the temporary key:

```json
{
  "apikey_id": null,
  "key_value": "eyJhbG..."
}
```

Use this temporary key either as a bearer token in the `Authorization` header, or as a query parameter on the WebSocket URL:

```
wss://eu2.rt.speechmatics.com/v2?jwt=<temporary-key>
```

For full details on authentication options, see the [Authentication](/get-started/authentication) page.

### 3. Pick and install a library

Check out our [JavaScript client](https://www.npmjs.com/package/@speechmatics/real-time-client) or [Python client](https://pypi.org/project/speechmatics-python/) to get started.

<Tabs groupId="language">
  <TabItem value="javascript" label="JavaScript">
    ```
    npm install @speechmatics/real-time-client @speechmatics/auth
    ```
  </TabItem>
  <TabItem value="python" label="Python">
    ```
    pip3 install speechmatics-python
    ```
  </TabItem>
</Tabs>


### 4. Insert your API key and run

Paste your API key into `YOUR_API_KEY` in the code.

Note how each SDK handles authentication differently:
- The **Python** example passes the API key directly via `auth_token` in `ConnectionSettings` (server-side approach).
- The **JavaScript** example generates a temporary key first using `createSpeechmaticsJWT` from `@speechmatics/auth`, then passes it to `client.start()`.

<Tabs groupId="language">
  <TabItem value="javascript" label="JavaScript">
    <CodeBlock language="javascript">
      {javascriptRadioExample}
    </CodeBlock>
  </TabItem>
  <TabItem value="python" label="Python">
    <CodeBlock language="python">
      {pythonRadioExample}
    </CodeBlock>
  </TabItem>
</Tabs>



## Transcript outputs

The API returns transcripts in JSON format. You can receive two types of output: [Final](#final-transcripts) and [Partial](#partial-transcripts) transcripts. Choose the type based on your latency and accuracy needs.

### Final transcripts

Final transcripts are the definitive result.
- They reflect the best transcription for the spoken audio.
- Once displayed, they are not updated.
- Words arrive incrementally, with some delay.

You control the latency and accuracy tradeoff [using the `max_delay` setting](/speech-to-text/realtime/output#latency) in your `transcription_config`.
Larger values of `max_delay` increase accuracy by giving the system more time to process audio context.

:::tip
Best for accurate, completed transcripts where some delay is acceptable
:::

### Partial transcripts

Partial transcripts are low-latency and can update later as more conversation context arrives.
- You must enable them using `enable_partials` in your `transcription_config`.
- Partials are emitted quickly (typically less than 500ms).
- The engine may revise them as more audio is processed.

You can combine partials with finals for a responsive user experience â€” show partials first, then replace them with finals as they arrive.

You control the latency and accuracy tradeoff using the [`max_delay` setting](/speech-to-text/realtime/output#latency) in your `transcription_config`.

:::tip
Use partials for: real-time captions, voice interfaces, or any case where speed matters
:::
